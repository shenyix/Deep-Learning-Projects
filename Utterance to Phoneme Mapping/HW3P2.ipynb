{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQTmd04rKz-d",
        "outputId": "6ee511e5-6ba4-4f2a-f06c-119f630d41bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 18 23:31:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9qZoIDcx-h",
        "outputId": "fa796189-798f-46cc-fdfc-5a766d0a240c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 34.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 86.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 80.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 96.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 95.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 101.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 92.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 98.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 99.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 99.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 102.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 97.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 100.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 100.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 99.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 97.7 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -q\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "d1a54961-3db0-40af-c831-9dbb52aa96f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"6b14f00e0515c5740d7e07042926608efdc6e526\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "4s52yBOvICPZ",
        "outputId": "551914d7-5ecb-41af-ad1c-208a2a1abbdf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-aec5fd884690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hw3p2-ablations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m### Project should be created in your wandb account\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;31m### Wandb Config for your run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = train_ ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "## Levenshtein\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "1bb8c261-b2d1-4d4a-e6ce-76d1a056cbab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.8\n",
            "  Downloading Levenshtein-0.20.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 100.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.20.8 python-Levenshtein-0.20.8 rapidfuzz-2.13.2\n",
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 22.35 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14102, done.        \n",
            "remote: Counting objects: 100% (415/415), done.        \n",
            "remote: Compressing objects: 100% (289/289), done.        \n",
            "remote: Total 14102 (delta 127), reused 383 (delta 112), pack-reused 13687        \n",
            "Receiving objects: 100% (14102/14102), 5.89 MiB | 17.34 MiB/s, done.\n",
            "Resolving deltas: 100% (8007/8007), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=03fead06a5224f39ddd413dc4626b47dfa5735bbdc2b0afb3e5b5301e7e1ba4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "/content/ctcdecode\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13405112 sha256=08c80e1548f0e808212aa64b2604110bce9fd5c13aee501c794c256823973faf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v7k9kgsl/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n",
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.12.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (4.1.1)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "f68be1e1-5222-4e96-9faa-eb2276f61602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import csv\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0v7wHRWrqH6",
        "outputId": "322b320b-9cb2-4b74-8717-e354f9f6207f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n"
          ]
        }
      ],
      "source": [
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \", # BLANK TOKEN\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:41]\n",
        "mapping = CMUdict_ARPAbet\n",
        "LABELS = ARPAbet[:41]\n",
        "print(len(LABELS))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, partition= \"train_tiny\"): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "\n",
        "        if partition==\"train_tiny\":\n",
        "          data_path='/content/hw3p2/train-clean-100'\n",
        "        elif partition==\"train_small\":\n",
        "          data_path='/content/hw3p2/train-clean-360'\n",
        "        else:\n",
        "          data_path='/content/hw3p2/dev-clean'\n",
        "        self.mfcc_dir = data_path+'/mfcc'\n",
        "        self.transcript_dir = data_path+'/transcript/raw'\n",
        "\n",
        "\n",
        "        mfcc_names = (os.listdir(self.mfcc_dir))\n",
        "        transcript_names =(os.listdir(self.transcript_dir))\n",
        "        mfcc_names.sort()\n",
        "        transcript_names.sort()\n",
        "\n",
        "\n",
        "        # Creating Array\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        self.PHONEMES = PHONEMES\n",
        "        def helper(x):\n",
        "          return self.PHONEMES.index(x)\n",
        "\n",
        "       \n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(0, len(mfcc_names)):\n",
        "       \n",
        "            mfcc =np.load(self.mfcc_dir+\"/\"+mfcc_names[i],allow_pickle=True)\n",
        "            \n",
        "            transcript = np.load(self.transcript_dir+\"/\"+transcript_names[i],allow_pickle=True)\n",
        "            transcript.reshape(1,transcript.shape[0])\n",
        "            transcript=transcript[(transcript != \"[SOS]\") & (transcript != \"[EOS]\")]\n",
        "            transcript=np.vectorize(helper)(transcript)\n",
        "           \n",
        "\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "        self.length = len(mfcc_names)\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.length\n",
        "       \n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "      \n",
        "        mfcc =  self.mfccs[ind]\n",
        "        transcript =self.transcripts[ind]\n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        batch_mfcc =[torch.from_numpy(x[0]) for x in batch]\n",
        "        batch_transcript = [torch.from_numpy(x[1]) for x in batch]\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc,batch_first=True)\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript,batch_first=True)\n",
        "        lengths_mfcc =[ len(x) for x in batch_mfcc]\n",
        "        lengths_transcript =[len(x) for x in batch_transcript]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "class AudioTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,data_path): \n",
        "        self.data_path =data_path\n",
        "        mfcc_names = (os.listdir(self.data_path))\n",
        "        mfcc_names.sort() \n",
        "        self.mfccs=[]\n",
        "\n",
        "\n",
        "        for i in range(0, len(mfcc_names)):\n",
        "            mfcc =np.load(self.data_path+\"/\"+mfcc_names[i],allow_pickle=True)\n",
        "\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "    def __len__(self):\n",
        "      return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "      return self.mfccs[ind]\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        batch_mfcc =[torch.from_numpy(x) for x in batch]\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc,batch_first=True)\n",
        "      \n",
        "        lengths_mfcc =[len(x) for x in batch_mfcc]\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFUgeZiN4s2n"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"beam_width\" : 30,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 70,\n",
        "    \"batch_size\":48\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        " # Increase if your device can handle it\n",
        "\n",
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "root = '/content/hw3p2' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "ad689718-4595-4a3b-de4e-7a44f8dd75a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW9S53OqVBt6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "\n",
        "\n",
        "\n",
        "train_data = AudioDataset(\"train_tiny\") \n",
        "val_data =  AudioDataset(\"validation\")\n",
        "test_data = AudioTestDataset(\"/content/hw3p2/test-clean/mfcc\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYkjekAiaExv",
        "outputId": "fe0c82fb-64e1-415e-b8a9-3b6f55ed649d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 28539, batches = 446\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 8, \n",
        "                                          batch_size=config['batch_size'], pin_memory= True, \n",
        "                                          shuffle= False,collate_fn=test_data.collate_fn)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 8,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= True,collate_fn=train_data.collate_fn)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 4,\n",
        "                                         batch_size=config['batch_size'], pin_memory= True,\n",
        "                                         shuffle= False,collate_fn=val_data.collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Batch size: \", config[\"batch_size\"])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "d8603496-4740-4dc5-c9f9-9cb4ef83df77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1657, 15]) torch.Size([64, 223]) torch.Size([64]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    \n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ-qQ_Sf-LIu",
        "outputId": "e6158163-7993-41c8-c821-860f7075bcc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "OUT_SIZE = len(LABELS)\n",
        "OUT_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.embed_size=15\n",
        "        self.hidden_size=680\n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            nn.Conv1d(self.embed_size, self.hidden_size, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(self.hidden_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(self.hidden_size, self.hidden_size, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(self.hidden_size),\n",
        "            nn.ReLU(inplace=True))\n",
        "        self.lstm = nn.LSTM(input_size=self.hidden_size,\n",
        "                            hidden_size=self.hidden_size,\n",
        "                            num_layers=3,\n",
        "                            bias=True,\n",
        "                            dropout=0.12,\n",
        "                            bidirectional=True) \n",
        "       \n",
        "        self.classification = nn.Sequential( torch.nn.Sequential(\n",
        "            nn.Linear(self.hidden_size*2, self.hidden_size*2),\n",
        "            nn.Linear(self.hidden_size*2,OUT_SIZE))\n",
        "        )\n",
        "        self.logSoftmax =nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        x_cnn_input = x.permute(0, 2, 1) \n",
        "        x_post_cnn = self.embedding(x_cnn_input)\n",
        "       \n",
        "        x_rnn_input = x_post_cnn.permute(2, 0, 1)\n",
        "\n",
        "\n",
        "        packed = pack_padded_sequence(x_rnn_input, lx,enforce_sorted=False)\n",
        "        output,hidden=self.lstm(packed)\n",
        "        out, out_lens = pad_packed_sequence(output, batch_first=True)\n",
        "        prob=self.classification(out)\n",
        "        prob=self.logSoftmax(prob)\n",
        "        prob = prob.permute(1, 0, 2)\n",
        "        return prob, out_lens\n",
        "\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGoiXd70tb5z",
        "outputId": "dec4b442-0ab6-475d-b6eb-c1404f497a25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Network().to(device)\n",
        "checkpoint =torch.load('/content/drive/MyDrive/11-785-s22/hw/lstm_360_512_4.pth')\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GWQgr1SLFpsH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "7YWhZf1SgeAQ",
        "outputId": "588602d9-85b3-4e54-be39-b563dea3b7a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-841738b28382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ],
      "source": [
        "summary(model,x.to(device),lx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "criterion = torch.nn.CTCLoss(blank=0)\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'],weight_decay=5e-5)\n",
        "\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "decoder = CTCBeamDecoder(\n",
        "    LABELS,\n",
        "    beam_width=config[\"beam_width\"],\n",
        "    num_processes=4,\n",
        "    blank_id=0,\n",
        "    log_probs_input=True\n",
        ")\n",
        "\n",
        "\n",
        "scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "### Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "# Use debug = True to see debug outputs\n",
        "def calculate_levenshtein(h, y, lh, ly, decoder, labels, debug = False):\n",
        "\n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here\n",
        "    h=torch.transpose(h, 0, 1)\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(h, seq_lens = lh)\n",
        "\n",
        "    \n",
        "\n",
        "    batch_size=len(h)\n",
        "    distance = 0 # Initialize the distance to be 0 initially\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "    for i in range(batch_size): \n",
        "      \n",
        "\n",
        "      sliced=beam_results[i, 0, : out_lens[i][0]]\n",
        "      cur_string=\"\".join([labels[x] for x in sliced])\n",
        "     \n",
        "      cur_y=\"\".join([labels[n] for n in y[i]])\n",
        "      \n",
        "      distance+=Levenshtein.distance(cur_string, cur_y[:ly[i]])\n",
        "    \n",
        "    distance /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "\n",
        "    \n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3IkMk3asvl-"
      },
      "outputs": [],
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "  for i, data in enumerate(train_loader):\n",
        "      \n",
        "     \n",
        "      x,y,lx,ly=data\n",
        "      x,y,lx,ly=x.to(device),y.to(device),lx,ly\n",
        "     \n",
        "      out,predict_length=model(x,lx)\n",
        "      \n",
        " \n",
        "      loss = criterion(out,y,predict_length, ly)\n",
        "      print(f\"loss: {loss}\")\n",
        "\n",
        "      distance = calculate_levenshtein(out, y,predict_length, ly, decoder, LABELS, debug = False)\n",
        "      print(f\"lev-distance: {distance}\")\n",
        "\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH0RAbCaMl9a"
      },
      "source": [
        "### Eval function\n",
        "Writing a function to do one round of evaluations will help make your code more modular, you can, however, choose to skip this if you'd like it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nqLiAmkMMBc"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "def evaluate(data_loader, model):\n",
        "    \n",
        "    dist = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    decoder= CTCBeamDecoder(LABELS, beam_width=config[\"beam_width\"],num_processes=4,blank_id=0,log_probs_input=True)\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    for i, (data) in enumerate(data_loader):\n",
        "       \n",
        "        x,y,lx,ly=data\n",
        "        x,y,lx,ly=x.to(device),y.to(device),lx,ly\n",
        "     \n",
        "        out,predict_length=model(x,lx)\n",
        "        \n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "          outputs,predict_length=model(x,lx)\n",
        "          loss = criterion(outputs,y,predict_length, ly)\n",
        "\n",
        "        train_loss += float(loss.item())\n",
        "        dist+= calculate_levenshtein(outputs, y,predict_length, ly, decoder, LABELS, debug = False)\n",
        "\n",
        "       \n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}\".format(float(dist/ (i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(train_loss / (i + 1))))\n",
        "        \n",
        "\n",
        "        batch_bar.update()\n",
        "        del x \n",
        "        del lx\n",
        "        del y\n",
        "        del ly \n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    dist = float(dist / len(data_loader))\n",
        "    total_loss = float(train_loss / len(data_loader))\n",
        "    \n",
        "    \n",
        "    return total_loss, dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGn17rLw9ChF"
      },
      "source": [
        "Again, writing a train step might help you code be more modular. You may choose to skip this and write the whole thing out in the training loop below if you so wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vH4QStLUjH8"
      },
      "outputs": [],
      "source": [
        "\n",
        "## we might need to place scheduler somewhere in the middle\n",
        "\n",
        "def train_step(train_loader, model, optimizer, criterion,scaler):\n",
        "   \n",
        "   \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "    train_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x,y,lx,ly=data\n",
        "        x,y,lx,ly=x.to(device),y.to(device),lx,ly\n",
        "        out,predict_length=model(x,lx)\n",
        "     \n",
        "        with torch.cuda.amp.autocast(): \n",
        "          \n",
        "          outputs,predict_length=model(x,lx)\n",
        "          loss = criterion(outputs,y,predict_length, ly)\n",
        "\n",
        "        train_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(train_loss / (i + 1))),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() \n",
        "        batch_bar.update()\n",
        "        del x\n",
        "        del y\n",
        "        del lx \n",
        "        del ly\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    train_loss /= float(train_loss/ len(train_loader))\n",
        "    \n",
        "\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY69hgxUXhTI"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "JR43E28rM9Ak",
        "outputId": "013a9c6d-8f07-4d73-bed2-51bb100c8807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.795542635658915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/446 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-04bd9209c8d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# one training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-5688a2436e24>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(train_loader, model, optimizer, criterion, scaler)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1302c684d3c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lx)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_rnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 773\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.32 GiB (GPU 0; 14.76 GiB total capacity; 11.50 GiB already allocated; 1.15 GiB free; 12.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "best_val_dist = checkpoint['val_dist']\n",
        "print(best_val_dist)\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "\n",
        "    # one training step\n",
        "    \n",
        "    train_loss=train_step(train_loader,model, optimizer, criterion,scaler)\n",
        "    if True:\n",
        "      val_loss, val_dist=evaluate(val_loader,model)\n",
        "      if True:\n",
        "        scheduler.step(val_dist)\n",
        "      print(\"Val Dist {:.04f}%\\t Val Loss {:.04f}\".format(val_dist, val_loss))\n",
        "      if val_dist < best_val_dist:\n",
        "        print(\"Saving model\")\n",
        "        torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'val_dist': val_dist, \n",
        "                  'epoch': epoch}, '/content/drive/MyDrive/11-785-s22/hw/lstm_360_512_4_fine_tune.pth')\n",
        "        best_val_dist = val_dist\n",
        "\n",
        "    \n",
        "    \n",
        "     \n",
        "    \n",
        "\n",
        "    # You may want to log some hyperparameters and results on wandb\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yltzwJ-ZLmn",
        "outputId": "70c07ea0-879b-4814-b676-fb67b0903480"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jze7tz6EgctO",
        "outputId": "65228364-ba12-4112-f753-0819a57afb41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "outputs": [],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "\n",
        "def make_output(h, lh):\n",
        "    \n",
        "\n",
        "    decoder= CTCBeamDecoder(LABELS, beam_width=config[\"beam_width\"]+20,num_processes=4,blank_id=0,log_probs_input=True)\n",
        "    h=torch.transpose(h, 0, 1)\n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(h, seq_lens=lh) \n",
        "    batch_size = len(h)\n",
        "\n",
        "    preds = []\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "        h_sliced =beam_results[i, 0, : out_seq_len[i][0]]\n",
        "        h_string =\"\".join([LABELS[x] for x in h_sliced])\n",
        "        preds.append(h_string)\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyszcvkMK0rT"
      },
      "outputs": [],
      "source": [
        "def predict(model,data_loader):\n",
        "  model.eval()\n",
        "  out=[]\n",
        "  for i,data in enumerate(data_loader):\n",
        "    x,lx=data\n",
        "    x,lx=x.to(device),lx\n",
        "    x,lx=model(x,lx)\n",
        "    preds=make_output(x, lx)\n",
        "    out+=preds\n",
        "  return out\n",
        "    \n",
        "    \n",
        "                                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWpAXnY-K0jS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d70dvu_lsMlv"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "# Write a function (predict) to generate predictions and submit the file to Kaggle\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "predictions = predict( model,test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJzvUvacimMa",
        "outputId": "e54f51fc-f47e-4748-e882-3e30a47ffc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 209k/209k [00:03<00:00, 60.3kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/hw3p2/test-clean/transcript/random_submission.csv')\n",
        "df.label = predictions\n",
        "\n",
        "df.to_csv('submission.csv', index = False)\n",
        "!kaggle competitions submit -c 11-785-f22-hw3p2 -f submission.csv -m \"Message\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "agmNBKf4JrLV",
        "NmuPk9J6L8dz",
        "Jmc6_4eWL2Xp",
        "kH0RAbCaMl9a",
        "qpYExu4vT4_g"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}